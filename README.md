# Perspectiefâ€¯LLMÂ +Â RAG Project

Endâ€‘toâ€‘end pipeline that turns a public website into a questionâ€‘answering assistant powered by a fineâ€‘tuned **Llamaâ€‘3.1 8B Instruct** model, a FAISS knowledge base and a lightweight RAG wrapper.

---

## ðŸ—ºï¸Â Repository structure

```
.
â”œâ”€â”€ Scrape-website/                  # website crawler â†’ JSONL dataset
â”‚Â Â  â””â”€â”€ scraper.py
â”œâ”€â”€ training/                 # fineâ€‘tuning assets
â”‚Â Â  â”œâ”€â”€ perspectief_training.py
â”œâ”€â”€ perspectief-llama3-rag-website/  # retrievalâ€‘augmented chat
â”‚Â Â  â”œâ”€â”€ build_kb.py
â”‚Â Â  â””â”€â”€ rag_chat.py
â””â”€â”€ README.md
```

---

## 0Â .Â Prerequisites

| Tool                                            | Version tested                          |
| ----------------------------------------------- | --------------------------------------- |
| Python                                          | Â 3.10Â â€“Â 3.12                            |
| CUDA                                            | Â 12.x (optional, CPU works but is slow) |
| PyTorch                                         | Â 2.2+                                   |
| [Unsloth](https://github.com/unslothai/unsloth) | Â 2025.5                                 |
| Transformers                                    | Â 4.41                                   |
| Sentenceâ€‘Transformers                           | Â 2.6                                    |
| FAISS                                           | Â 1.7                                    |

Install everything in one go:

```bash
pip install -r requirements.txt     # provided
```

> **GPU note:** the training script uses 4â€‘bit QLoRA and runs comfortably on a single RTXÂ 4090 or A6000 (â‰ˆâ€¯24â€¯GBÂ vRAM).

---

## 1Â .Â Scrape the website

```bash
python scraper/crawl_perspectief.py \
       --base https://perspectief.eu/ \
       --out  data/perspectief_raw.json
```

The crawler:

* follows internal links only
* extracts **URL**, **title**, **main content**
* skips duplicates & junk pages

---

## 2Â .Â Convert raw pages â†’ fineâ€‘tune dataset

The helper `scraper/dataset-processor.py` reads the raw pages and heuristically derives **Question / Answer** pairs (H2Â â†’Â Q, paragraphÂ â†’Â A). The resulting JSONâ€‘Lines file uses the schema your model expects:

```jsonc
{
  "URL": "â€¦",
  "Question": "What is the total cost for the 4â€‘hour plenary session, and is there a discount for colleagues?",
  "Answer":   "The plenary session costs â‚¬595 (excluding VAT)â€¦",
  "DetailedContext": "The â‚¬595 fee appliesâ€¦"
}
```

Run it:

```bash
python scraper/dataset-processor.py \
       --in  data/perspectief_raw.json \
       --out data/perspectief_dataset-1.jsonl
```

ðŸ’¡Â **Manual pass recommended:** skim the JSONL and fix typosâ€”garbage in, garbage out!

---

## 3Â .Â Fineâ€‘tune Llamaâ€‘3Â with Unsloth

The training script below (already in `training/`) applies 4â€‘bit QLoRA to **Metaâ€‘Llamaâ€‘3.1â€‘8Bâ€‘Instruct**. It formats every row as:

```
<|user|>
{Question}

Context:
{DetailedContext}
<|assistant|>
{Answer}<|end|>
```

### 3.1Â Â Launch training

```bash
python training/perspectief_training.py \
  --output_dir training/checkpoints/perspectief-llama3 \
  --epochs 6 \
  --batch  2 \
  --lr     2e-5
```

* Early stopping isnâ€™t enabled;Â monitor loss and abort if it plateaus.
* The script automatically splits 10â€¯% for validation.

### 3.2Â Â Result

```
training/checkpoints/perspectief-llama3/
 â”œâ”€ adapter_model.safetensors   # LoRA weights
 â”œâ”€ adapter_config.json
 â”œâ”€ tokenizer.json / tokenizer_config.json
 â””â”€ â€¦
```

---

## 4Â .Â Build the knowledge base (FAISS)

```bash
python rag/build_kb.py \
       --data      data/perspectief_dataset-1.jsonl \
       --index_dir kb
```

* Uses **allâ€‘MiniLMâ€‘L6â€‘v2** embeddings (swapable via `--embedder`).
* Normalises embeddings so `IndexFlatIP` â‰ˆ cosine similarity.

Output:

```
kb/
 â”œâ”€ faiss.index   # vectors
 â””â”€ meta.pkl      # original rows, aligned with vectors
```

---

## 5Â .Â Run Retrievalâ€‘Augmented Generation

```bash
python rag/rag_chat.py \
       --question "" \
       --top_k    3
```

Under the hood:

1. **Embed** the question â†’ cosine search in FAISS.
2. **Assemble prompt** in the exact fineâ€‘tune template (user/context/assistant).
3. **Generate** with the adapter on top of the base model.

Typical output:

```
Context passages used:
--------------------------------------------------------------------------------
The â‚¬595 fee applies to each primary participantâ€¦
[source] https://...
--------------------------------------------------------------------------------
A: De plenaire sessie kost â‚¬595 (excl. btw) en een tweede collega krijgt 40Â % kortingâ€¦
```

---

## 6Â .Â Evaluation & QA checks

| Check                 | Command                                       |
| --------------------- | --------------------------------------------- |
| **Retriever sanity**  | `python rag/quick_test.py --question "â€¦"`     |
| **Exactâ€‘match score** | see `evaluation/simple_em.py`                 |
| **Live chat**         | integrate `rag_chat.py` as a FastAPI endpoint |

---

## 7Â .Â Troubleshooting

### FAISS `AttributeError: numpy_array_to_float_array`

Youâ€™re on FAISSÂ â‰¥Â 1.7. The fixed `build_kb.py` already passes a plain `np.ndarray`â€”make sure you pulled latest.

### CUDA OOM during training

* Reduce `--batch` or enable gradientâ€‘checkpointing (already on).
* Fit can be resumed from the last checkpoint.

### Model ignores retrieved context

* Increase `--top_k` in `rag_chat.py`.
* Split very long `DetailedContext` strings (<Â 512 tokens ideal).
